# **反事实遗憾最小化**
——[《An Introduction to Counterfactual Regret Minimization》](http://cs.gettysburg.edu/~tneller/modelai/2013/cfr/index.html#:~:text=An%20Introduction%20to%20Counterfactual%20Regret%20Minimization%20In%202000%2C,wished%20they%20had%20made%20the%20moves%20on%20average%29.)自译自用

## **1 动机**

在2020年，Hart和Mas-Colell引入了一个重要的博弈理论算法——*regret matching*。玩家通过追踪过去博弈的遗憾（regret）来达到均衡的博弈，这让未来的博弈偏向正遗憾（positive regret）。所用到的技术不仅简单而且符合直觉，而且它引发了计算机游戏的一场革命，包括年度计算机扑克竞赛在内的绝大多数下注游戏。

由于这个算法相对较新，基本上很少有入门以以遗憾为基础算法（regret-based algorithms）的课程材料提供给该领域研究人员和实践者。这些材料代表了一些合适的入门材料提供给告诫计算机课程的本科生、研究生、感兴趣的研究人员和有雄心的实践人员。

- 在第二部分中，我们引入了玩家遗憾（player regret）的概念，描述了遗憾匹配算法（regret matching algorithm），用语言描述的程序语言展示了石头剪刀布的例子，还给出了一些相关的练习。
- 反事实遗憾最小化（Counterfactual Regret Minimization CFT）在第三部分中由库恩扑克（Kuhn Poker）例子引出。提供了一个CFR练习的代码去计算1-die-versus-1-die Dudo最优博弈。
- 在第四部分中，我们简单地提到了清除近似最优计算策略，这样能够在很多情况下都改善结果。
- 第五部分包括了一个CFR在可重复状态游戏中的前沿应用（例如通过非完美召回抽象），它能够将CFR的计算复杂度从指数幂减少到线性的。在这里，我们用我们独立设计的游戏（Liar Die）去论证这种算法的应用。然后我们建议读者应用这种技术，用在有三个主张存储的1-die-versus-1-die Dudo中。
- 在第六部分中，我们简要讨论了一个开放的研究问题：在可能的均衡策略中，我们怎么样计算一个策略，使得能够最优探索对手的错误？我们邀请读者去修改Liar Die的代码例子去获得对这个有趣问题的insight。
- 最后，在第七部分中，我们提出了一些有关在线学习（continued learning）更多的挑战问题和路径。

## **2 博弈中的遗憾**

在这个部分中，我们描述一种方法，计算机自己通过模拟游戏过程，利用过去博弈选择的遗憾去告知未来的选择。我们通过大家很熟悉的石头剪刀布游戏（RPS）来开始。在给出了博弈论的基本定义之后，我们讨论遗憾匹配算法，呈现出一种能够最小期望遗憾的算法计算策略。利用这种算法，我们提供一个有效的例子去学习RPS策略和一些相关练习。

### **2.1 石头剪刀布**
RPS是一种两人的游戏，玩家同时做出三个手势之一：石头（握紧的拳头）、剪刀（两根手指伸出，其余手指握紧）、布（手掌摊开）。用每个收拾都有可能赢、输或平局（玩家出相同手势）。石头胜剪刀，因为石头可以破坏剪刀，剪刀胜布，因为剪刀能剪开布，布胜石头，因为布能包裹石头。

通常玩家会同步喊出四字口号：“石头！剪刀！布！出！”，前三句口号需要摇摆伸出的拳头，最后一次口号同时出手势。

### **2.2 博弈理论定义**

玩一个游戏最优的或是完美的是什么意思呢？假设最大化胜利减去失败的次数取决于对手如何去玩这个游戏的话，这个问题本身有任何含义吗？在这个部分中，我们从博弈论中引入一些基本概念，并考虑对于最优博弈的解的概念（solution concepts）。在这里我们遵循[12]的符号和术语.

首先，我们定义一个普通形式的博弈，用**元组$(N, A, u)$**来表示，其中：

- $N=\{1,...,n\}$是一个有限集合，代表有$n$个玩家
- $S_i$是一个有限集合，表示玩家$i$的行为或选择
- $A=S_i\times \cdots \times S_n$是所有玩家同时行动的可能组合，每个可能的组合称为一个行动组合（action profile）
- $u$是一个函数映射，对每个玩家，都将每个行为组合映射到一组收益向量$(u_1,...,u_n)=u(x_1,...,x_n)$（a vector of utilities）上，我们称第$i$个玩家的收益（payoff/reward/utility）为$u_i$

一个普通形式的博弈通常被称作一个“one-shot game”，因为每一个玩家只做单个选择。我们可以用一张n维的表格来表示这样的博弈，其中每个维度都有一列或一行来表示单个玩家的行为，每个表项对应单个行为组合（每个玩家的单步行为的组合）。对于RPS游戏来说，收益表如下：

| 收益向量 |  石头     | 布           |      剪刀       |
| :---: | :---:  | :---:       |      :---:    |
| **石头** | 0, 0   |   -1, 1     |      1, -1    |
| **布**   | 1, -1  |   0, 0      |      -1, 1    |
| **剪刀** |  -1, 1 |   1, -1     |      0, 0     |

每个表项都有形式$(u_1, u_2)$，通常情况下，行是玩家1，列是玩家2。举个例子，在RPS中，$A=\{(R,R), (R,P),\cdots, (S, P), (S, S)   \}$

一个普通的博弈形式是**零和博弈（zero-sum）**，如果每个收益向量的价值加起来为0的话。**常和博弈（constant-sum）**是每个收益向量加起来为常数的博弈，可能会通过加一个虚拟的玩家来把它重新表示成零和博弈的形式，这个虚拟玩家会进行一次虚拟的行动使得总是让那个常数的相反数作为收益。

如果一个玩家选择一个行为的概率为1的话，我们就称这个玩家采取了一个纯策略（pure strategy）；如果一个玩家采取的行为至少有两种概率都为正的话，我们称这个玩家采取了一种混合策略（mixed strategy）。

我们用$\sigma$指代一种混合策略，并定义$\sigma_i(s)$为第$i$个玩家选择某种策略$s\in S_i$的概率。传统来讲，-$i$一般指的是除了玩家$i$以外的对手们，所以在一个两人游戏中$S_{-i} = S_{3-i},i=1,2$，为了计算游戏中**一个智能体的期望收益**，对每个玩家采取相应行为组合策略概率的乘积再乘以收益求和即可：$$\mathbb{E}[u_i(\sigma_i, \sigma_{-i})]=\Sigma p(X,Y)u(X,Y)=\sum_{s\in S_i} \sum_{s'\in S_{-i}} \sigma_i(s)\sigma_{-i}(s') u_i(s, s').$$

这是在两人游戏中的收益计算。对于玩家$i$的一个最佳的响应策略是，给定所有其他玩家策略(***问题：这个策略是当前的还是全局的***)，最大化$i$的期望收益$u_i$，当每位玩家都以相对于其他玩家的最佳响应策略进行游戏的话，那么策略的组合称作一个**纳什均衡策略**（Nash equilibrium）。没有玩家能够期望去通过独立改变策略去改变这场博弈。

考虑这场性别博弈：

Monica-Gary
|       |  M     | G           | 
| :---: | :---:  | :---:       |
| M     | 2, 1   |   0, 0     |  
| G       | 0, 0  |   1, 2      | 

Monica是行玩家，Gary是列玩家，假设他们将出去约会，并需要选择一项活动（例如电影、餐厅等）。Gary想去看一场足球赛（G）而Monica想看电影（M）。他们都倾向于一起去同一项活动，但是他们不想选择彼此的偏好活动。

假设Monica始终选择M，Gary最好选择M，并且没有想法去单方面地偏离那个纯策略；同样的，如果Gary始终选择G，Minica没有想法去单方面偏离那个纯策略。那么收益始终是（2，1）或者（1，2）。所以（M,M）和（G,G）是两种纯纳什均衡组合。然而，在混合策略中也可以实现纳什均衡。即当每个玩家看到别人的策略时无动于衷，不影响自己的行为选择的时候，也能达到均衡。

如果Monica对 Gary的选择无动于衷的话会发生什么？令$\sigma_{Gary}(M)=x$作为Gary选择看电影的概率，那么Monika的条件期望收益分别$是2x+0(1-x)$（Monica选M）和$0x+1(1-x)$（Monica选G）。由于即使Monica知道Gary的选择之后还是无所谓选G或M，所以这两个期望收益应当相等，解得$x=\frac{1}{3}$。同理，当Monika以三分之一的概率选择G的时候，Gary无论选择G还是M都能够达到最大且相同的收益，因此，他们可以分别采用混合策略$(\frac{2}{3},\frac{1}{3})$和$(\frac{1}{3},\frac{2}{3})$，此时达到纳什均衡。两个人都对改变没有变化，注意到这些纳什均衡策略对于玩家而言会收获不同的期望收益。有三种纳什均衡，（G,G）,(M,M),$(\frac{1}{3}G+\frac{2}{3}M,\frac{2}{3}G+\frac{1}{3}M)$

纳什均衡是一个solution concept解的概念。另一个更加常用的解概念是相关均衡correlated equilibrium。现在，想象两个玩家都能够从第三方接触到一些类型的随机信号。玩家接收到自己的信号信息，不知道彼此的信号信息。如果玩家互相关联地使用信号，也即每个信号对应一个**行动组合action profile**，也即每个玩家的行为，并且每个玩家都不会单方面地通过改变信号到行为的映射来获得收益，那么玩家们就形成了一个**相关均衡**。

纳什均衡都是相关均衡，但是相关均衡的概念会更加通用，并且会容纳更重要的解。再次考虑Gary和Monica的问题。作为一个简单的信号例子signal example，想象一个公平的掷硬币。玩家能完成一个合作的行为，由此举个例子，掷硬币的结果对应于选择Gary还是Monica。达成一个这样的均衡以后，两个玩家都不能通过单方面改变signal信号到strategy策略的映射，并且两个玩家都平均获得1.5的收益。(**WHY**)

-   纳什均衡和相关均衡的相同点：只要其他人服从均衡指定的推荐，那么自己服从推荐就是最优的
-   不同点：相关均衡时每个参与人自己的推荐和其他人的推荐可以是相关的，而纳什均衡无论自己选择什么，对方的混合策略都是独立于自己从而是给定的。

### **2.3 遗憾匹配和遗憾最小化**

假设我们正在用石头剪刀布赌钱，每个玩家都在桌子上放一美元。如果有胜者，那么胜者就会拿走桌上两人的美元。否则没有胜者，玩家保留他们的美元。进一步假设我们出石头的时候我们的对手出布赢了，导致我们输掉了亿美元。让我们的收益称为我们的网络收益或损失。那么我们的收益是-1。我们出布和出剪刀对付对手的布的时候，收益分别是0和1。

我们后悔没出布，但是我们更后悔没出剪刀，因为我们的相对收益会变得更多。我们在这里定义某个行为的遗憾regret（类似loss），为选择该种行动的收益与实际选择收益之差，相对于其他玩家的固定选择。

对于实际行为组合$a\in A$而言。进一步而言，让$u(s_i',s_{-i})$表示第$i$个玩家用行动$s'_i$去代替实际行为$s_i$的收益，那么在这一步过后，玩家$i$没有采取行动$s'_i$的遗憾为$u(s'_i,s_{-i})-u(a)$。当$s_i'=s_i$的时候，遗憾为0。

对于这个例子而言，我们后悔没有出布的遗憾为1，后悔没出剪刀的遗憾为2。

这怎样告知未来的游戏游玩呢？通常来说，一个人通常会倾向于选择过去最遗憾没有选择的，但是一个人不会希望完全是可被预测的导致完全可被利用，一种达到这样目的的方式是通过**遗憾匹配regret matching**：智能体的行为用与正遗憾成比例的分布去随机选择。正遗憾代表着一个智能体所经历过没有选择每个行为的相对损失程度。在我们的例子中，我们没有遗憾选择石头，但是我们对于没有选择布和剪刀分别有着1和2的遗憾值。采用遗憾匹配的方法，我们接下来选择我们的下一个行为与正遗憾成比例的分布，因此选择石头剪刀布的概率分别为：$0, \frac{2}{3}, \frac{1}{3}$，这是一组归一化的正遗憾，称作遗憾匹配策略组合（regret-matching strategy profile，这是一组离散分布，与之相对的是action profile，这是一组具体的值），也就是说会除以正遗憾的总和。

现在我们假设在下一场游戏中，我们以三分之二的概率碰巧选择了剪刀，而我们的对手选择出石头，对于这场游戏，我们对于石头剪刀布分别有1，0，2的遗憾值。把这些累加到我们上一次的遗憾中。我们能够得到石头剪刀布的**累计遗憾cumulative regrets**：1，2，3。所以下一次遗憾匹配的混合策略为：$(\frac{1}{6},\frac{2}{6},\frac{3}{6})$。

理想状况下，我们希望最小化随时间变化的期望遗憾。然而这种练习本身对于最小化我们的期望遗憾值是不充分的。假设即使你是游戏中的对手，你现在充分理解了正在应用的遗憾匹配算法，那么你就可以完成和我同样的计算，观察到任何一个我们对于这场游戏的偏好，并利用那种偏好。在我们学到那种偏好遗憾的时候，伤害就已经造成了，我们那些新的遗憾会同样地被利用。

然而，存在一种计算上下文，能够让遗憾匹配算法通过自我训练来最小化期望遗憾，算法呈现如下：

-   对于每个玩家，初始化所有累计遗憾为0.
-   迭代如下：
    -   计算一个遗憾匹配策略组合（如果存在一名玩家他的所有遗憾值都是非正的，那么他采取均匀选择策略）。
    -   把策略组合加到策略组合总和里。
    -   根据策略组合选择每名玩家的行为组合。
    -   计算玩家的遗憾值
    -   把玩家的遗憾值加到玩家的累计遗憾中。
-   返回平均策略组合，也就是策略 组合之和除以迭代次数。

在充分迭代之后，这个过程会收敛到相关均衡。在下一小节中，我们提供了一个应用该算法的石头剪刀布的例子。

### **2.4 例子：石头剪刀布**

现在我们呈现一个遗憾匹配的例子去计算最佳响应策略。在石头剪刀布中，遗憾匹配的延伸拓展会导向一个平衡，在本节最后留作练习。

我们从常量和变量的定义开始

```java
//<definitions>:
public static final int ROCK=0, PAPER=1, SCISSORS=2, NUM_ACTIONS=3;
public static final Random random=new Random();
double[] regretSum = new double[NUM_ACTIONS],
		 strategy  = new double[NUM_ACTIONS],
		 strategySum = new double[NUM_ACTIONS],
		 oppStrategy = {0.4, 0.3, 0.3};
```

尽管在代码中没有用到，但是我们还是任意为石头剪刀布赋值为021（英文版为RPS）。这样的行为索引对应到索引的长度NUM\_ACTIONS为3。我们创建一个随机数生成器用来在一个混合策略中选择行动。最后，我们分配数组空间regretSum去存储我们的累计行动遗憾，通过遗憾匹配的生成策略strategy，以及所有策略生成的总和strategySum。

遗憾匹配会采用与之前没有选择的行为的正遗憾成比例的分布去选择行为。为了利用遗憾匹配计算混合策略，我们先通过拷贝所有正遗憾并求和开始。然后在策略组合项中做第二次选择。如果有至少一个行为有正遗憾，我们就用所有正遗憾的和归一化遗憾值。在这个上下文中归一化意味着我们确保数组中的数和为1并且因此表示了在混合策略中选择相应行为的概率。

```java
//<get current mixed strategy through regret-matching>
private double[] getStrategy(){
    double normalizingSum = 0;
    for (int a = 0; a < NUM_ACTIONS; a++){
        strategy[a] = regretSum[a] > 0 ? regretSum[a] : 0;
        normalizingSum += strategy[a];
    }
    for (int a = 0; a < NUM_ACTIONS; a++){
        if (normalizingSum > 0)
            strategy[a] /= normalizingSum;
        else
            strategy[a] = 1.0 / NUM_ACTIONS;
        strategySum[a] += strategy[a]
    }
    return strategy;
}
```

一些读者可能不熟悉选择运算符（也就是，条件？条件为真时执行的表达式：条件为假时执行的表达式）。这是一种条件语句的表达式，在C、C++、Java中都有。

注意到归一化和normalizingSum可能是非正的。在这样的情况下，我们让策略均匀化，给每个行为一个均等的概率（代码第10行）。

然后返回策略组合。给定任意一组这样的策略，玩家可以根据该策略选择行动。假设我们有一个混合策略（0.2，0.5，0.3），如果从0到1划分这些数字，划分会在.2和0.2+0.5=0.7上。随机数会生成在[0,1)中，[0,.2)，[.2,.7), [.7,1)，落在这三个区间中的某一个，进而。。选择对应的行动。

通常来讲，假设有行为对应与概率为：$a_i\sim p_i$，概率前缀和为：$c_i=\Sigma_{j=0}^{i}p_j$，那么对于一个随机生成的数$r$，选择行为$i$当且仅当落在$[c_i, c_{i+1})$中。

行为很简单就能计算，首先生成一个浮点随机数，初始化行为索引为0，累积概率为0。如果我们将要到达最后一个行为索引的时候（NUM\_ACTIONS-1），那必然是被选择的行为，所以只要行为索引不是我们的最后一个，我们就把新的概率加到累积概率中，如果找到$r$的第一个小于的前缀和，跳出循环，否则继续循环。

```java
//<Get random action according to mixed-strategy distribution>
public int getAction(double[] strategy){
    double r = random.nextDouble();
    int a = 0;
    double cumulativeProbability = 0;
    while (a < NUM_ACTIONS - 1){
        cumulativeProbability += strategy[a];
        if (r < cumulativeProbability)
            break;
        a++;
    }
    return a;
}
```

利用上述模块，我们现在可以建立训练算法：

```java
//<train>
public void train(int iterations){
    double[] actionUtility = new double[NUM_ACTIONS];
    for (int i = 0; i < iterations; i++){
        <Get regret-matched mixed-strategy actions>
        <Compute action utilities>
        <Accumulate action regrets>
    }
}
```

对于一个给定的迭代次数，我们计算我们的遗憾匹配，混合策略行动，和相应的行动收益，并累积相应的遗憾值。

为了选择由玩家选择的行为，我们计算当前的遗憾匹配策略，并用这去选择每位玩家的行为。因为采用混合策略，所以使用相同策略并不会导致选择相同的行为。

```java
//<Get regret-matched mixed-strategy actions>
double[] strategy = getStrategy();
int myAction = getAction(strategy);
int otherAction = getAction(oppStrategy);
```

接下来，我们从玩家执行自己的行动来计算每个可能可能行为的收益：

```java
//<Compute action utilities>
actionUtility[otherAction] = 0;
actionUtility[otherAction == NUM_ACTIONS-1 ? 0 : otherAction + 1] = 1;
actionUtility[otherAction == 0 ? NUM_ACTIONS-1 : otherAction - 1] = -1;
```

最后，对于每个行动，我们都计算遗憾值，也即行为的期望收益和已经选择的行为收益之差，然后我们把它加到累积遗憾中。

```java
//<Accumulate action regrets>
for (int a = 0; a < NUM_ACTIONS; a++){
    regretSum[a] += actionUtility[a] - actionUtility[myAction];
}
```

对于训练中每次单独的迭代，遗憾都可能会暂时地以不好的形式偏离，即在混合策略中有负的遗憾相加然后就再也不会被选择。由于遗憾相加的原因，因此每次迭代策略都是高度不稳定的。收敛到最佳遗憾策略的是在所有迭代后的**平均策略average strategy**。它类似getStrategy的计算，但是不用关心负值累加。

```java
//<Get average mixed strategy across all trining iterations>
public double[] getAverageStrategy(){
    double[] avgStrategy = new double[NUM_ACTIONS];
    double normalizingSum = 0;
    for (int a = 0; a < NUM_ACTIONS; a++){
        normalizingSum += strategySum[a];
    }
    for (int a = 0; a < NUM_ACTIONS; a++)
        if (normalizingSum > 0)
            avgStrategy[a] = strategySum[a] / normalizingSum;
    	else
            avgStrategy[a] = 1.0 / NUM_ACTIONS;
    return avgStrategy;
}
```

总计算包括建立一个trainer对象，执行给定次数的迭代，比如1, 000, 000，并打印结果平均策略：

```java
//<Main method initializing computation>
public static void main(String[] args){
    RPSTrainer trainer = new RPSTrainer();
    trainer.train(1000000);
    System.out.printIn(Arrays.toString(trainer.getAverageStrategy()));
}
```

把所有这些元素结合起来，我们就创建了一个利用遗憾匹配算法的石头剪刀布训练器，它随着迭代次数增加能够近似最小化期望遗憾。

```java
//<RPSTrainer.java>
import java.util.Arrays;
import java.util.Random;

public class RPSTrainer{
    <Definitions>
    <Get current mixed strategy through regret-matching>
    <Get random action according to mixed-strategy distribution>
    <Train>
    <Get average mixed strategy across all training iterations>
    <Main method initializing computation>
}
```

用遗憾匹配计算的平均策略是一种在对手出手策略固定时的最小化遗憾策略。换句话说，这是最优响应。在这种情况下，对手采用策略（0.4，0.3，0.3）。这可能不会很明显，但是相对于任意混合策略而言，始终有一个纯粹的最佳相应策略。在这种情况下，什么样的pure strategy会成为最佳响应呢？这与RPSTrainer的结果能够呼应吗。

### **2.5 练习：石头剪刀布均衡**

在石头剪刀布和每个两玩家零和游戏中：当两个玩家都采用遗憾最小化去更新他们的策略的时候，这对平均策略会随着迭代次数趋于无穷而收敛到纳什均衡。在每次迭代过程中，两个玩家都会像如上所述的那样更新遗憾值，接下来两个玩家都会根据遗憾值计算他们各自更新的策略、

修改上面RPSTrainer程序，一名玩家采用遗憾匹配，另外一名玩家固定出牌概率。计算并打印唯一的均衡策略结果是什么。

**结论：会收敛到一个纯粹策略pure strategy=**$max\{opponent\ mixed\ strategy\}$。

### **2.6 练习：Colonel Blotto**

Colonel Blotto和他的敌人Boba Fett在打仗。每位指挥官都有S名士兵，美味士兵可以被指派到某个战场N上（战场数N小于S）。自然地，这些指挥官不能沟通，因此独立地指挥各自的士兵。任意数量的士兵可以被指派到任意战场上，包括0。如果某位指挥官在一个战场上指派了比另一位指挥官更多的士兵，那么这名指挥官占领了这个战场。最终胜利的指挥官是占领更多战场的那个。举个例子，当（S, N）=（10，4）时，Colonel Blotto可能会指派（2，2，2，4），而Boba Fett可能会选择（8，1，1，0）。在这种情况下，Colonel Blotto会获胜，因为他在三个战场上都有更多士兵。如果两个指挥官都占领了同样的战场，则平局。

写一个程序，它能够让每位玩家交替地用遗憾匹配算法去找到一个纳什均衡，设（S, N）=（5，3）。一些建议：在开始迭代之前，首先思考对于玩家而言所有的纯粹策略；接下来为每个纯粹策略赋值。纯粹策略可以表示为字符、对象或者一个三数字的数：设置一个全局的数组，索引对应于策略的ID。接下来写一个各自指挥官使用的单独的函数，返回给定策略ID的收益。



[12] Kevin Layton-Brown and Yoav Shoham. Essentials of Game Theory: A Concise, Multidisciplinary
Introduction. Morgan and Claypool Publishers, 2008.